'''
#!/usr/bin/env bash
pip install -r requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8333

'''

from fastapi import FastAPI, HTTPException
from fastapi.responses import PlainTextResponse, HTMLResponse

import toml,os 
import httpx,base64 
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from cachetools import cached, TTLCache
from functools import lru_cache

from typing import Tuple,List

app = FastAPI(title="Free Node from WebSites", version="1.1")

urls = dict(
    mibei="https://www.mibei77.com" ,
    nodefree="https://nodefree.me" ,
    changfeng="https://www.cfmem.com"  ,
    v2rayshare="https://v2rayshare.net"  ,
    openrunner="https://freenode.openrunner.net", # è¿™ä¸ªçš„V2Rayé“¾æ¥è®¿é—®ä¸äº† 
    # ä¸‹é¢2ä¸ªé“¾æ¥æ˜¯ä¸€æ ·çš„ï¼Œå…¶ä¸­è¿˜åŒ…æ‹¬äº†ä¸Šé¢çš„3ä¸ª
    clashnode="https://clashnode.cc"  ,
    freeclashnode="https://www.freeclashnode.com"  ,
)

#=========== Tools =====================

@lru_cache(maxsize=15)
def fetch_text_sync(url: str) -> str:
    with httpx.Client(timeout=15) as cli:
        r = cli.get(url)
        r.raise_for_status()
        return r.text
    
# @lru_cache(maxsize=10)
async def fetch_text_async(url: str) -> str:
    async with httpx.AsyncClient(timeout=15) as cli:
        r = await cli.get(url)
        r.raise_for_status()
        return r.text


# è®¾ç½®60åˆ†é’Ÿç¼“å­˜
@cached(cache=TTLCache(maxsize=2, ttl=3600))
def get_urls_mibei(url):
    r = httpx.get(url)
    
    # ä½¿ç”¨å…¼å®¹æ€§æ›´å¥½çš„è§£æå™¨
    soup = BeautifulSoup(r.text, "html.parser")
    articles = soup.select("div#main article")
    
    return [
        a["href"] 
        for article in articles 
        if (a := article.find("a", href=True))
    ]

@lru_cache(maxsize=2)
def get_sub_mibei(url: str):
    r = httpx.get(url)
    # soup = BeautifulSoup(r.content, "lxml")
    soup = BeautifulSoup(r.content, "html.parser")
    body = soup.find("div", id="post-body")
    txt = yaml = ""
    for p in body.find_all("p"):
        t = p.get_text(strip=True)
        if t.endswith(".txt"): txt = t
        elif t.endswith(".yaml"): yaml = t
    return txt, yaml

# è®¾ç½®60åˆ†é’Ÿç¼“å­˜
@cached(cache=TTLCache(maxsize=2, ttl=3600))
def get_urls_changfeng(url):
    r = httpx.get(url)
    
    # ä½¿ç”¨å…¼å®¹æ€§æ›´å¥½çš„è§£æå™¨
    soup = BeautifulSoup(r.text, "html.parser")
    articles = soup.select("div#main h2")
    
    return [
        a["href"] 
        for article in articles 
        if (a := article.find("a", href=True))
    ]

def extract_url_from_string(text: str) -> str:
    """åŸé€»è¾‘ï¼šä»ä¸€æ®µæ–‡å­—é‡Œæå–ç¬¬ä¸€ä¸ª https?:// é“¾æ¥"""
    import re
    m = re.search(r'https?://\S+', text)
    return m.group(0) if m else ''


@lru_cache(maxsize=2)
def get_sub_changfeng(url: str) -> Tuple[str, str, str, str, str]:
    html = httpx.get(url, timeout=15).text
    # soup = BeautifulSoup(html, 'lxml')
    soup = BeautifulSoup(html, 'html.parser')

    post = soup.select_one('div#post-body')
    if not post: raise 

    # 1. Base64 èŠ‚ç‚¹å—
    base64ss = ''
    # pre = post.select_one('h2:-soup-contains("èŠ‚ç‚¹Base64") + pre')
    pre = post.find("h2", string="èŠ‚ç‚¹Base64").find_next("pre")
    # print(pre)
    if pre:
        base64ss = pre.get_text(strip=True)

    # 2. è®¢é˜…é“¾æ¥å—
    sub_div = post.select_one('h2:-soup-contains("è®¢é˜…é“¾æ¥") + div')
    if not sub_div:
        return (base64ss, ) + ('', ) * 4

    # æŒ‰é¡ºåºæå– 4 ä¸ªé“¾æ¥ï¼ˆv2ray/clash/mihomo/singboxï¼‰
    links = []
    for d in sub_div.select(':scope > div'):   # åªå–ç›´æ¥å­ div
        links.append(extract_url_from_string(d.get_text(' ')))

    # ç¼ºä½è‡ªåŠ¨è¡¥ç©ºä¸²
    links.extend([''] * 4)
    v2ray, clash, mihomo, singbox = links[:4]

    return base64ss, v2ray, clash, mihomo, singbox


# è®¾ç½®60åˆ†é’Ÿç¼“å­˜
@cached(cache=TTLCache(maxsize=2, ttl=3600))
def get_urls_openrunner(url):
    r = httpx.get(url)
    
    # ä½¿ç”¨å…¼å®¹æ€§æ›´å¥½çš„è§£æå™¨
    soup = BeautifulSoup(r.text, "html.parser")
    articles = soup.find_all("a", class_="mb-5")
    print(len(articles),articles)
    
    return [
        urljoin(url, a["href"])
        for a in articles 
    ]

@lru_cache(maxsize=2)
def get_sub_openrunner(url: str):
    r = httpx.get(url)
    # soup = BeautifulSoup(r.content, "lxml")
    soup = BeautifulSoup(r.content, "html.parser")
    # body = soup.find("code")
    txt = yaml = ""
    for p in soup.find_all("code"):
        t = p.get_text(strip=True)
        if t.endswith(".txt"): txt = t
        elif t.endswith(".yaml"): yaml = t
    return txt, yaml

@cached(cache=TTLCache(maxsize=2, ttl=3600))
def get_urls_nodefree(url:str) -> List[str]:
    """
    è·å– NodeFree é¦–é¡µæ–‡ç« é“¾æ¥ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    é»˜è®¤å…¥å£ï¼šhttps://nodefree.org
    """
    # soup = _get_soup(entry)
    soup = BeautifulSoup(httpx.get(url, timeout=15).text, "html.parser")
    # h2 ä¸‹ç›´æ¥ a æ ‡ç­¾
    links = [urljoin(url, a["href"])
             for h2 in soup.select("section#boxmoe_theme_container h3")
             if (a := h2.find("a")) and a.get("href")]
    return links


@lru_cache(maxsize=2)
def get_sub_nodefree(url: str) -> Tuple[str, str]:
    soup = BeautifulSoup(httpx.get(url, timeout=15).text, "html.parser")
    post = soup.select_one("section#boxmoe_theme_container")
    if not post: raise 

    # ä¸€æ¬¡æ€§æ‹¿åˆ°æ‰€æœ‰æ®µè½
    txt = yaml = ""
    for p in post.select("p"):
        text = p.get_text(" ", strip=True)
        if text.endswith(".txt"): txt = text
        elif text.endswith(".yaml"): yaml = text
    return txt, yaml


@cached(cache=TTLCache(maxsize=2, ttl=3600))
def get_urls_v2rayshare(url:str) -> List[str]:
    """
    è·å– NodeFree é¦–é¡µæ–‡ç« é“¾æ¥ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    é»˜è®¤å…¥å£ï¼šhttps://v2rayshare.com
    """
    soup = BeautifulSoup(httpx.get(url, timeout=15).text, "html.parser")
    articles = soup.select("div article")
    
    return [
        a["href"] 
        for article in articles 
        if (a := article.find("a", href=True))
    ]


@lru_cache(maxsize=2)
def get_sub_v2rayshare(url: str) -> Tuple[str, str]:
    
    soup = BeautifulSoup(httpx.get(url, timeout=15).text, "html.parser")
    if not soup: raise 

    # ä¸€æ¬¡æ€§æ‹¿åˆ°æ‰€æœ‰æ®µè½
    txt = yaml = ""
    for p in soup.select("p"):
        text = p.get_text(" ", strip=True)
        if text.endswith(".txt"): txt = text
        elif text.endswith(".yaml"): yaml = text
        
    return txt, yaml


@cached(cache=TTLCache(maxsize=2, ttl=3600))
def get_urls_clashnode(url:str) -> List[str]:
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
        }
    
    soup = BeautifulSoup(httpx.get(url, timeout=30,headers=headers).text, "html.parser")
    articles = soup.select("div.panel-body")
    
    urls = [
        a["href"] 
        for article in articles 
        if (a := article.find("a", href=True))
    ]
    return [ urljoin(url, a) for a in urls if a.endswith(".htm")]

@lru_cache(maxsize=2)
def get_sub_clashnode(url: str):
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
        }
    
    soup = BeautifulSoup(httpx.get(url, timeout=30,headers=headers).text, "html.parser")
    if not soup: raise

    # ä¸€æ¬¡æ€§æ‹¿åˆ°æ‰€æœ‰æ®µè½
    txt = []
    yaml = []
    json_list = []
    for p in soup.select("p"):
        text = p.get_text(" ", strip=True)
        if text.endswith(".txt"): txt.append(text)
        elif text.endswith(".yaml"): yaml.append(text)
        elif text.endswith(".json"): json_list.append(text)
        
    return txt, yaml,json_list


#=========== Route =====================
@app.get("/{type}/mb", 
         response_class=PlainTextResponse,
         summary="ç±³è´",tags=["ç±³è´",],
         description=f"ç±³è´çš„V2Ray|ClashèŠ‚ç‚¹:  {urls['mibei']}",         
         )
async def route_mibei(    type: str ):
    url = urls["mibei"]

    urls_list = get_urls_mibei(url)
    urlv,urlc = get_sub_mibei(urls_list[0])

    if type == 'v': return fetch_text_sync(urlv)
    elif type == 'c': return fetch_text_sync(urlc)

    raise HTTPException(status_code=404, detail="Invalid node type: {type}")

#=========== Route =====================
@app.get("/{type}/cf", 
         response_class=PlainTextResponse,
         summary="é•¿é£",tags=["é•¿é£",],
         description=f"é•¿é£çš„V2Ray|ClashèŠ‚ç‚¹:  {urls['changfeng']}",         
         )
async def route_changfeng(    type: str ):
    url = urls["changfeng"]

    urls_list = get_urls_changfeng(url)
    base64ss, v2ray, clash, mihomo, singbox = get_sub_changfeng(urls_list[0])

    if type == 'v': return fetch_text_sync(v2ray)
    elif type == 'c': return fetch_text_sync(clash)

    raise HTTPException(status_code=404, detail="Invalid node type: {type}")

#=========== Route =====================
@app.get("/{type}/vs", 
         response_class=PlainTextResponse,
         summary="V2RayShare",tags=["V2RayShare",],
         description=f"V2RayShareçš„V2Ray|ClashèŠ‚ç‚¹:  {urls['v2rayshare']}",         
         )
async def route_varyshare(    type: str ):
    url = urls["v2rayshare"]

    urls_list = get_urls_v2rayshare(url)
    v2ray, clash = get_sub_v2rayshare(urls_list[0])

    if type == 'v': return fetch_text_sync(v2ray)
    elif type == 'c': return fetch_text_sync(clash)

    raise HTTPException(status_code=404, detail="Invalid node type: {type}")

#=========== Route =====================
@app.get("/{type}/nf", 
         response_class=PlainTextResponse,
         summary="NodeFree",tags=["NodeFree",],
         description=f"NodeFreeçš„V2Ray|ClashèŠ‚ç‚¹:  {urls['nodefree']}",         
         )
async def route_nodefree(    type: str ):
    url = urls["nodefree"]

    urls_list = get_urls_nodefree(url)
    v2ray, clash = get_sub_nodefree(urls_list[0])

    if type == 'v': return fetch_text_sync(v2ray)
    elif type == 'c': return fetch_text_sync(clash)

    raise HTTPException(status_code=404, detail="Invalid node type: {type}")

#=========== Route =====================
@app.get("/{type}/or", 
         response_class=PlainTextResponse,
         summary="OpenRunner",tags=["OpenRunner",],
         description=f"OpenRunnerçš„V2Ray|ClashèŠ‚ç‚¹:  {urls['openrunner']}",         
         )
async def route_openrunner(    type: str ):
    url = urls["openrunner"]

    urls_list = get_urls_openrunner(url)
    v2ray, clash = get_sub_openrunner(urls_list[0])

    if type == 'v': return fetch_text_sync(v2ray)
    elif type == 'c': return fetch_text_sync(clash)

    raise HTTPException(status_code=404, detail="Invalid node type: {type}")

#=========== Route =====================
@app.get("/{type}/cn", 
         response_class=PlainTextResponse,
         summary="ClashNode",tags=["ClashNode",],
         description=f"ClashNodeçš„V2Ray|ClashèŠ‚ç‚¹:  {urls['clashnode']}",         
         )
async def route_clashnode(    type: str ):
    url = urls["clashnode"]

    urls_list = get_urls_clashnode(url)
    v2ray_list, clash_list,_ = get_sub_clashnode(urls_list[0])

    if type == 'v': return fetch_text_sync(v2ray_list[0])
    elif type == 'c': return fetch_text_sync(clash_list[0])

    raise HTTPException(status_code=404, detail="Invalid node type: {type}")


def load_nodes(fpath:str = "nodes.toml"):
    """    åŠ è½½èŠ‚ç‚¹ä¿¡æ¯    """
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(fpath):
        print(f"èŠ‚ç‚¹æ–‡ä»¶ {fpath} ä¸å­˜åœ¨")
        raise FileNotFoundError(f"æ–‡ä»¶ {fpath} ä¸å­˜åœ¨")
    
    # è¯»å–ç°æœ‰é…ç½®æ–‡ä»¶
    try: 
        with open(fpath, "r", encoding="utf-8") as f:
            cfg = toml.load(f)
        return cfg
    except Exception as e:
        print(f"åŠ è½½èŠ‚ç‚¹é…ç½®æ–‡ä»¶å¤±è´¥: {e}") 
        return {}

@app.get("/sb/{tag}", 
         response_class=PlainTextResponse,
         summary="SingBox Nodes",tags=["SingBox",],
         description=f"QiQçš„èŠ‚ç‚¹(singbox)", 
         )
async def route_mibei(tag: str,t:str ):
    cfg = load_nodes()

    if t!=cfg['TOKEN']:
        raise HTTPException(status_code=401, detail="Invalid token")

    if tag not in [ "all"] and tag not in cfg:
        raise HTTPException(status_code=404, detail=f"Invalid node tag: {tag}")

    nodes_list = []
    if tag=='all':
        for k,v in cfg.items():
            if k.strip() in ['TOKEN']: continue 
            
            # nodes = v.strip().split('\n')
            lines = [line.strip() for line in v.splitlines() if line.strip()]
            nodes_list.extend(lines)
    else: 
        # nodes_list = cfg[tag].strip().split('\n')
        nodes_list = [line.strip() for line in cfg[tag].splitlines() if line.strip()]
    
    nodes_list = list(set(nodes_list)) # å»é‡ 
    result_str = '\n'.join(nodes_list).replace('hy2:','hysteria2:') 
    return base64.b64encode(result_str.encode()).decode()


#=========== Index =====================
@app.get("/", response_class=HTMLResponse,tags=['é¦–é¡µ'])
async def index():
    return """
    <h1>Free Node from Web</h1>
    <p><a href="/docs">ğŸ“˜ API Docs</a> | <a href="/redoc">ğŸ“™ ReDoc</a></p>
    <ul>
        <li><a href="/c/mb">Clash - ç±³è´</a></li>
        <li><a href="/v/mb">V2Ray - ç±³è´</a></li>
        <li><a href="/c/cf">Clash - é•¿é£</a></li>
        <li><a href="/v/cf">V2Ray - é•¿é£</a></li>
        <li><a href="/c/nf">Clash - NodeFree</a></li>
        <li><a href="/v/nf">V2Ray - NodeFree</a></li>
        <li><a href="/c/vs">Clash - V2RayShare</a></li>
        <li><a href="/v/vs">V2Ray - V2RayShare</a></li>
        <li><a href="/c/or">Clash - OpenRunner</a></li>
        <li><a href="/v/or">V2Ray - OpenRunner</a></li>
    </ul>
    """

#=========== Main =====================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8300,reload=True)